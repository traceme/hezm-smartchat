#!/usr/bin/env python3
"""
Test script for the dialogue engine
Usage: uv run python backend/test_dialogue_engine.py
"""

import asyncio
import sys
import time
import json
from typing import List, Dict, Any

# Add backend directory to Python path
sys.path.insert(0, '.')

from backend.services.dialogue_service import dialogue_service
from backend.services.llm_service import LLMProvider
from backend.core.config import get_settings

async def test_dialogue_engine():
    """Test the complete dialogue engine functionality"""
    print("ğŸš€ Testing Dialogue Engine...")
    
    settings = get_settings()
    print(f"ğŸ“¡ Embedding API: {settings.embedding_api_url}")
    print(f"ğŸ¤– Embedding Model: {settings.embedding_model}")
    print(f"ğŸ—„ï¸  Qdrant URL: {settings.qdrant_url}")
    
    # Test queries
    test_queries = [
        "What is machine learning?",
        "How does FastAPI work?",
        "Explain vector databases",
        "What is natural language processing?",
        "How to use Python for web development?"
    ]
    
    print(f"ğŸ“ Test queries: {len(test_queries)}")
    
    try:
        # Test 1: Basic vector search functionality
        print("\nğŸ” Test 1: Vector search functionality...")
        
        query = test_queries[0]
        start_time = time.time()
        
        fragments = await dialogue_service.search_relevant_fragments(
            query=query,
            limit=10
        )
        
        search_time = time.time() - start_time
        print(f"âœ… Vector search completed")
        print(f"â±ï¸  Search time: {search_time:.3f}s")
        print(f"ğŸ“¦ Fragments found: {len(fragments)}")
        
        if fragments:
            print(f"ğŸ“Š Top fragment score: {fragments[0]['similarity_score']:.4f}")
            print(f"ğŸ“„ Sample content: {fragments[0]['content'][:100]}...")
        
        # Test 2: Context generation
        print("\nğŸ” Test 2: Context generation...")
        
        if fragments:
            context, citations = await dialogue_service.generate_context_from_fragments(
                fragments=fragments[:5],
                max_context_length=2000
            )
            
            print(f"âœ… Context generation completed")
            print(f"ğŸ“ Context length: {len(context)}")
            print(f"ğŸ“š Citations count: {len(citations)}")
            print(f"ğŸ“„ Context preview: {context[:200]}...")
        
        # Test 3: Prompt preparation
        print("\nğŸ” Test 3: Prompt preparation...")
        
        if fragments:
            prompt = await dialogue_service.prepare_dialogue_prompt(
                query=query,
                context=context,
                conversation_history=[
                    {"role": "user", "content": "Hello"},
                    {"role": "assistant", "content": "Hi! How can I help you?"}
                ]
            )
            
            print(f"âœ… Prompt preparation completed")
            print(f"ğŸ“ Prompt length: {len(prompt)}")
            print(f"ğŸ“„ Prompt preview: {prompt[:300]}...")
        
        # Test 4: Check available LLM providers
        print("\nğŸ” Test 4: LLM providers check...")
        
        available_providers = []
        for provider in LLMProvider:
            try:
                config = dialogue_service.llm_service.providers.get(provider)
                if config and config.get("api_key"):
                    available_providers.append(provider)
                    print(f"âœ… {provider.value}: Available ({config['model']})")
                else:
                    print(f"âš ï¸  {provider.value}: Not configured")
            except Exception as e:
                print(f"âŒ {provider.value}: Error - {str(e)}")
        
        print(f"ğŸ“Š Available providers: {len(available_providers)}")
        
        # Test 5: Full query processing (if any provider is available)
        if available_providers:
            print("\nğŸ” Test 5: Full query processing...")
            
            test_provider = available_providers[0]
            start_time = time.time()
            
            try:
                result = await dialogue_service.process_query(
                    query=query,
                    model_preference=test_provider.value
                )
                
                processing_time = time.time() - start_time
                
                print(f"âœ… Query processing completed")
                print(f"â±ï¸  Processing time: {processing_time:.3f}s")
                print(f"ğŸ¤– Model used: {result['model_used']}")
                print(f"ğŸ“¦ Fragments found: {result['fragments_found']}")
                print(f"ğŸ“š Citations: {result['fragments_used']}")
                print(f"ğŸ“„ Response preview: {result['response'][:200]}...")
                
                if result.get('usage'):
                    print(f"ğŸ”¢ Token usage: {result['usage']}")
                
            except Exception as e:
                print(f"âŒ Query processing failed: {str(e)}")
        
        else:
            print("\nâš ï¸  Test 5: Skipped (no LLM providers configured)")
        
        # Test 6: Streaming query processing (if available)
        if available_providers:
            print("\nğŸ” Test 6: Streaming query processing...")
            
            try:
                chunks_received = 0
                start_time = time.time()
                
                async for chunk in dialogue_service.process_query_stream(
                    query="What is Python?",
                    model_preference=available_providers[0].value
                ):
                    chunks_received += 1
                    chunk_type = chunk.get("type", "unknown")
                    
                    if chunk_type == "status":
                        print(f"ğŸ“‹ Status: {chunk.get('message')}")
                    elif chunk_type == "citations":
                        print(f"ğŸ“š Citations received: {len(chunk.get('citations', []))}")
                    elif chunk_type == "chunk":
                        print("ğŸ“", end="", flush=True)  # Show progress
                    elif chunk_type == "final":
                        streaming_time = time.time() - start_time
                        print(f"\nâœ… Streaming completed")
                        print(f"â±ï¸  Streaming time: {streaming_time:.3f}s")
                        print(f"ğŸ“¦ Total chunks: {chunks_received}")
                        print(f"ğŸ“„ Final response length: {len(chunk.get('response', ''))}")
                    elif chunk_type == "error":
                        print(f"\nâŒ Streaming error: {chunk.get('error')}")
                        break
                
            except Exception as e:
                print(f"âŒ Streaming test failed: {str(e)}")
        
        else:
            print("\nâš ï¸  Test 6: Skipped (no LLM providers configured)")
        
        # Test 7: Performance benchmark
        print("\nğŸ” Test 7: Performance benchmark...")
        
        benchmark_queries = test_queries[:3]  # Test first 3 queries
        total_search_time = 0
        
        for i, bq in enumerate(benchmark_queries):
            start_time = time.time()
            
            bench_fragments = await dialogue_service.search_relevant_fragments(
                query=bq,
                limit=5
            )
            
            search_time = time.time() - start_time
            total_search_time += search_time
            
            print(f"ğŸ“Š Query {i+1}: {search_time:.3f}s, {len(bench_fragments)} fragments")
        
        avg_search_time = total_search_time / len(benchmark_queries)
        print(f"âš¡ Average search time: {avg_search_time:.3f}s")
        print(f"ğŸš€ Search rate: {1/avg_search_time:.1f} queries/sec")
        
        print("\nğŸ‰ All tests completed!")
        
        # Summary
        print("\nğŸ“‹ Test Summary:")
        print(f"âœ… Dialogue engine functional")
        print(f"ğŸ“¡ Vector search: Working")
        print(f"ğŸ¤– Embedding service: Working")
        print(f"ğŸ’¬ LLM providers: {len(available_providers)} available")
        print(f"âš¡ Search performance: {avg_search_time:.3f}s avg")
        
        if available_providers:
            print(f"ğŸ”„ Full pipeline: Working")
            print(f"ğŸ“º Streaming: Working")
        else:
            print(f"âš ï¸  Full pipeline: Limited (no LLM API keys)")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # Cleanup
        await dialogue_service.close()

def main():
    """Main function"""
    print("=" * 60)
    print("ğŸ”¬ SmartChat Dialogue Engine Test")
    print("=" * 60)
    
    # Run async test
    success = asyncio.run(test_dialogue_engine())
    
    if success:
        print("\nâœ… All tests passed! Dialogue engine is working properly.")
        return 0
    else:
        print("\nâŒ Tests failed! Please check configuration and dependencies.")
        return 1

if __name__ == "__main__":
    exit(main()) 